{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import copy\n",
    "\n",
    "def get_ordered_booking_file_paths(root_dir):\n",
    "    # List to hold file paths with corresponding dates\n",
    "    files_with_dates = []\n",
    "\n",
    "    # Walk through the root directory\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        try:\n",
    "            dir_date = datetime.strptime(os.path.basename(dirpath), '%d-%m-%Y')\n",
    "        except ValueError:\n",
    "            continue \n",
    "\n",
    "        # Find all files that match the pattern\n",
    "        for filename in filenames:\n",
    "            if (filename.startswith('booking_1j')) and filename.endswith('.csv'):\n",
    "                files_with_dates.append((os.path.join(dirpath, filename), dir_date))\n",
    "\n",
    "    # Sort the list of files by the date part\n",
    "    files_with_dates.sort(key=lambda x: x[1])\n",
    "    print('Ordered Files :',files_with_dates)\n",
    "    # Extract and return only the file paths in sorted order\n",
    "    sorted_file_paths = [file_path for file_path, _ in files_with_dates]\n",
    "    return sorted_file_paths\n",
    "\n",
    "def fixed_date_range(year):\n",
    "    start_date = datetime(year, 4, 1)\n",
    "    end_date = datetime(year, 4, 3)\n",
    "    return start_date, end_date\n",
    "\n",
    "def parse_date(date_str):\n",
    "    if  date_str==\"\" or any(c.isalpha() for c in date_str):\n",
    "        return None\n",
    "    date_formats = [\n",
    "        '%Y-%m-%d %H:%M:%S',  \n",
    "        '%Y-%m-%d',          \n",
    "        '%d-%m-%Y',       \n",
    "        '%d/%m/%Y'            \n",
    "    ]\n",
    "    \n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    print('Date:' , date_str)\n",
    "    print(date_str == \"\")\n",
    "    raise ValueError(f\"Date format not recognized: {date_str}\")\n",
    "def add_days(nb_days, date_initiale_str):\n",
    "    date_initiale = parse_date(date_initiale_str)\n",
    "    nouvelle_date = date_initiale + timedelta(days=nb_days)\n",
    "    return nouvelle_date.strftime('%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered Files : [('./MAEVAS/27-02-2023/maeva_cleaned_27_02_2023.csv', datetime.datetime(2023, 2, 27, 0, 0)), ('./MAEVAS/06-03-2023/maeva_06_03_2023_cleaned.csv', datetime.datetime(2023, 3, 6, 0, 0)), ('./MAEVAS/13-03-2023/maeva_13_03_2023_cleaned.csv', datetime.datetime(2023, 3, 13, 0, 0)), ('./MAEVAS/20-03-2023/maeva_20_03_2023_cleaned.csv', datetime.datetime(2023, 3, 20, 0, 0)), ('./MAEVAS/27-03-2023/maeva_27_03_2023_cleaned.csv', datetime.datetime(2023, 3, 27, 0, 0)), ('./MAEVAS/03-04-2023/maeva_03_04_2023_cleaned.csv', datetime.datetime(2023, 4, 3, 0, 0)), ('./MAEVAS/10-04-2023/maeva_10_04_2023_cleaned.csv', datetime.datetime(2023, 4, 10, 0, 0)), ('./MAEVAS/17-04-2023/maeva_21_04_2023_cleaned.csv', datetime.datetime(2023, 4, 17, 0, 0)), ('./MAEVAS/24-04-2023/maeva_24_04_2023_cleaned.csv', datetime.datetime(2023, 4, 24, 0, 0)), ('./MAEVAS/08-05-2023/maeva_08_05_2023_cleaned.csv', datetime.datetime(2023, 5, 8, 0, 0)), ('./MAEVAS/15-05-2023/maeva_15_05_2023_cleaned.csv', datetime.datetime(2023, 5, 15, 0, 0)), ('./MAEVAS/22-05-2023/maeva_22_05_2023_cleaned.csv', datetime.datetime(2023, 5, 22, 0, 0)), ('./MAEVAS/29-05-2023/maeva_29_05_2023_cleaned.csv', datetime.datetime(2023, 5, 29, 0, 0)), ('./MAEVAS/05-06-2023/maeva_05_06_2023_cleaned.csv', datetime.datetime(2023, 6, 5, 0, 0)), ('./MAEVAS/12-06-2023/maeva_12_06_2023_cleaned.csv', datetime.datetime(2023, 6, 12, 0, 0)), ('./MAEVAS/19-06-2023/maeva_cleaned_19_06_2023.csv', datetime.datetime(2023, 6, 19, 0, 0)), ('./MAEVAS/26-06-2023/maeva_26_06_2023_cleaned.csv', datetime.datetime(2023, 6, 26, 0, 0)), ('./MAEVAS/03-07-2023/maeva_cleaned_03_07_2023.csv', datetime.datetime(2023, 7, 3, 0, 0)), ('./MAEVAS/10-07-2023/maeva_cleaned_10_07_2023.csv', datetime.datetime(2023, 7, 10, 0, 0)), ('./MAEVAS/17-07-2023/maeva_cleaned_17_07_2023.csv', datetime.datetime(2023, 7, 17, 0, 0)), ('./MAEVAS/24-07-2023/maeva_cleaned_24_07_2023.csv', datetime.datetime(2023, 7, 24, 0, 0)), ('./MAEVAS/31-07-2023/maeva_cleaned_31_07_2023.csv', datetime.datetime(2023, 7, 31, 0, 0)), ('./MAEVAS/07-08-2023/maeva_cleaned_07_08_2023.csv', datetime.datetime(2023, 8, 7, 0, 0)), ('./MAEVAS/14-08-2023/maeva_cleaned_14_08_2023.csv', datetime.datetime(2023, 8, 14, 0, 0)), ('./MAEVAS/21-08-2023/maeva_cleaned_21_08_2023.csv', datetime.datetime(2023, 8, 21, 0, 0)), ('./MAEVAS/28-08-2023/maeva_cleaned_28_08_2023.csv', datetime.datetime(2023, 8, 28, 0, 0)), ('./MAEVAS/04-09-2023/maeva_cleaned_04_09_2023.csv', datetime.datetime(2023, 9, 4, 0, 0)), ('./MAEVAS/11-09-2023/maeva_cleaned_11_09_2023.csv', datetime.datetime(2023, 9, 11, 0, 0)), ('./MAEVAS/18-09-2023/maeva_cleaned_18_09_2023.csv', datetime.datetime(2023, 9, 18, 0, 0)), ('./MAEVAS/25-09-2023/maeva_cleaned_25_09_2023.csv', datetime.datetime(2023, 9, 25, 0, 0)), ('./MAEVAS/02-10-2023/maeva_cleaned_02_10_2023.csv', datetime.datetime(2023, 10, 2, 0, 0)), ('./MAEVAS/16-10-2023/maeva_cleaned_16_10_2023.csv', datetime.datetime(2023, 10, 16, 0, 0)), ('./MAEVAS/23-10-2023/maeva_cleaned_23_10_2023.csv', datetime.datetime(2023, 10, 23, 0, 0)), ('./MAEVAS/30-10-2023/maeva_cleaned_30_10_2023.csv', datetime.datetime(2023, 10, 30, 0, 0)), ('./MAEVAS/06-11-2023/maeva_cleaned_06_11_2023.csv', datetime.datetime(2023, 11, 6, 0, 0)), ('./MAEVAS/13-11-2023/maeva_cleaned_13_11_2023.csv', datetime.datetime(2023, 11, 13, 0, 0)), ('./MAEVAS/20-11-2023/maeva_cleaned_20_11_2023.csv', datetime.datetime(2023, 11, 20, 0, 0)), ('./MAEVAS/27-11-2023/maeva_cleaned_27_11_2023.csv', datetime.datetime(2023, 11, 27, 0, 0)), ('./MAEVAS/04-12-2023/maeva_cleaned_04_12_2023.csv', datetime.datetime(2023, 12, 4, 0, 0)), ('./MAEVAS/11-12-2023/maeva_cleaned_11_12_2023.csv', datetime.datetime(2023, 12, 11, 0, 0)), ('./MAEVAS/18-12-2023/maeva_cleaned_18_12_2023.csv', datetime.datetime(2023, 12, 18, 0, 0)), ('./MAEVAS/25-12-2023/maeva_cleaned_25_12_2023.csv', datetime.datetime(2023, 12, 25, 0, 0)), ('./MAEVAS/01-01-2024/maeva_cleaned_01 _01_2024.csv', datetime.datetime(2024, 1, 1, 0, 0)), ('./MAEVAS/08-01-2024/maeva_cleaned_08_01_2024.csv', datetime.datetime(2024, 1, 8, 0, 0)), ('./MAEVAS/15-01-2024/maeva_cleaned_15_01_2024__ - maeva_cleaned_15_01_2024__.csv', datetime.datetime(2024, 1, 15, 0, 0)), ('./MAEVAS/29-01-2024/maeva_cleaned_29_01_2024.csv', datetime.datetime(2024, 1, 29, 0, 0)), ('./MAEVAS/05-02-2024/maeva_cleaned_05_02_2024.csv', datetime.datetime(2024, 2, 5, 0, 0)), ('./MAEVAS/12-02-2024/maeva_cleaned_12_02_2024.csv', datetime.datetime(2024, 2, 12, 0, 0)), ('./MAEVAS/19-02-2024/maeva_cleaned_19_02_2024.csv', datetime.datetime(2024, 2, 19, 0, 0)), ('./MAEVAS/04-03-2024/maeva_cleaned_04_03_2024.csv', datetime.datetime(2024, 3, 4, 0, 0)), ('./MAEVAS/11-03-2024/maeva_cleaned_11_03_2024.csv', datetime.datetime(2024, 3, 11, 0, 0)), ('./MAEVAS/18-03-2024/maeva_cleaned_18_03_2024.csv', datetime.datetime(2024, 3, 18, 0, 0)), ('./MAEVAS/25-03-2024/maeva_cleaned_25_03_2024.csv', datetime.datetime(2024, 3, 25, 0, 0)), ('./MAEVAS/01-04-2024/maeva_cleaned_01_04_2024.csv', datetime.datetime(2024, 4, 1, 0, 0)), ('./MAEVAS/08-04-2024/maeva_cleaned_08_04_2024.csv', datetime.datetime(2024, 4, 8, 0, 0)), ('./MAEVAS/15-04-2024/maeva_cleaned_15_04_2024.csv', datetime.datetime(2024, 4, 15, 0, 0)), ('./MAEVAS/22-04-2024/maeva_cleaned_22_04_2024.csv', datetime.datetime(2024, 4, 22, 0, 0)), ('./MAEVAS/06-05-2024/maeva_cleaned_06_05_2024.csv', datetime.datetime(2024, 5, 6, 0, 0)), ('./MAEVAS/13-05-2024/maeva_cleaned_13_05_2024.csv', datetime.datetime(2024, 5, 13, 0, 0)), ('./MAEVAS/20-05-2024/maeva_cleaned_20_05_2024.csv', datetime.datetime(2024, 5, 20, 0, 0)), ('./MAEVAS/27-05-2024/maeva_cleaned_27_05_2024.csv', datetime.datetime(2024, 5, 27, 0, 0)), ('./MAEVAS/03-06-2024/maeva_cleaned_03_06_2024.csv', datetime.datetime(2024, 6, 3, 0, 0)), ('./MAEVAS/10-06-2024/maeva_cleaned_10_06_2024.csv', datetime.datetime(2024, 6, 10, 0, 0)), ('./MAEVAS/17-06-2024/maeva_cleaned_17_06_2024.csv', datetime.datetime(2024, 6, 17, 0, 0)), ('./MAEVAS/24-06-2024/maeva_cleaned_24_06_2024.csv', datetime.datetime(2024, 6, 24, 0, 0)), ('./MAEVAS/01-07-2024/maeva_cleaned_01_07_2024.csv', datetime.datetime(2024, 7, 1, 0, 0)), ('./MAEVAS/08-07-2024/maeva_cleaned_08_07_2024.csv', datetime.datetime(2024, 7, 8, 0, 0)), ('./MAEVAS/15-07-2024/maeva_cleaned_15_07_2024.csv', datetime.datetime(2024, 7, 15, 0, 0)), ('./MAEVAS/22-07-2024/maeva_cleaned_22_07_2024.csv', datetime.datetime(2024, 7, 22, 0, 0)), ('./MAEVAS/29-07-2024/maeva_cleaned_29_07_2024.csv', datetime.datetime(2024, 7, 29, 0, 0)), ('./MAEVAS/05-08-2024/maeva_cleaned_05_08_2024.csv', datetime.datetime(2024, 8, 5, 0, 0)), ('./MAEVAS/12-08-2024/maeva_cleaned_12_08_2024.csv', datetime.datetime(2024, 8, 12, 0, 0)), ('./MAEVAS/19-08-2024/maeva_cleaned_19_08_2024.csv', datetime.datetime(2024, 8, 19, 0, 0))]\n",
      "./MAEVAS/27-02-2023/maeva_cleaned_27_02_2023.csv\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe8 in position 202: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(csvfile)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(file_path)\n\u001b[0;32m---> 26\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtypologie\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocalite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mannonces\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/csv.py:110\u001b[0m, in \u001b[0;36mDictReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;66;03m# Used only for its side effect.\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfieldnames\u001b[49m\n\u001b[1;32m    111\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader\u001b[38;5;241m.\u001b[39mline_num\n",
      "File \u001b[0;32m/usr/lib/python3.11/csv.py:97\u001b[0m, in \u001b[0;36mDictReader.fieldnames\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fieldnames \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fieldnames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe8 in position 202: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Main execution block\n",
    "#root_directory = './Booking'\n",
    "\n",
    "root_directory = './MAEVAS'\n",
    "\n",
    "file_paths = get_ordered_booking_file_paths(root_directory)\n",
    "\n",
    "for path in file_paths:\n",
    "    #print(path)\n",
    "    continue\n",
    "\n",
    "# Load data from files\n",
    "annonces_par_fichier = []\n",
    "missing_dates_dict = {}\n",
    "\n",
    "for file_path in file_paths:\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    annonces = {}\n",
    "    #print('Processing :', file_path)\n",
    "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        print(file_path)\n",
    "        for row in reader:\n",
    "            key = f\"{row['typologie']}-{row['nom']}-{row['localite']}\"\n",
    "            if key not in annonces:\n",
    "                annonces[key] = []\n",
    "            annonces[key].append({\n",
    "                'date_debut': parse_date(row['date_debut']),\n",
    "                'date_fin': parse_date(row['date_fin']),\n",
    "                'row': row\n",
    "            })\n",
    "    annonces_par_fichier.append(annonces)\n",
    "\n",
    "# Process missing keys and update dates\n",
    "print('Processing missing dates...')\n",
    "year_of_interest = 2024\n",
    "start_date, end_date = fixed_date_range(year_of_interest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_days(nb_days, date_initiale_str):\n",
    "    date_initiale = parse_date(date_initiale_str)\n",
    "    nouvelle_date = date_initiale + timedelta(days=nb_days)\n",
    "    return nouvelle_date.strftime('%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1   and File 0\n",
      "File 2   and File 1\n",
      "File 3   and File 2\n",
      "File 4   and File 3\n",
      "File 5   and File 4\n",
      "File 6   and File 5\n",
      "File 7   and File 6\n",
      "File 8   and File 7\n",
      "File 9   and File 8\n",
      "File 10   and File 9\n",
      "File 11   and File 10\n",
      "File 12   and File 11\n",
      "File 13   and File 12\n",
      "File 14   and File 13\n",
      "File 15   and File 14\n",
      "File 16   and File 15\n",
      "File 17   and File 16\n",
      "File 18   and File 17\n",
      "File 19   and File 18\n",
      "File 20   and File 19\n",
      "File 21   and File 20\n",
      "File 22   and File 21\n",
      "File 23   and File 22\n",
      "File 24   and File 23\n",
      "File 25   and File 24\n",
      "File 26   and File 25\n",
      "File 27   and File 26\n",
      "File 28   and File 27\n",
      "File 29   and File 28\n",
      "File 30   and File 29\n",
      "File 31   and File 30\n",
      "File 32   and File 31\n",
      "File 33   and File 32\n",
      "File 34   and File 33\n",
      "File 35   and File 34\n",
      "File 36   and File 35\n",
      "File 37   and File 36\n",
      "File 38   and File 37\n",
      "File 39   and File 38\n",
      "File 40   and File 39\n",
      "File 41   and File 40\n",
      "File 42   and File 41\n",
      "File 43   and File 42\n",
      "File 44   and File 43\n",
      "File 45   and File 44\n",
      "File 46   and File 45\n",
      "File 47   and File 46\n",
      "File 48   and File 47\n",
      "File 49   and File 48\n",
      "File 50   and File 49\n",
      "File 51   and File 50\n",
      "File 52   and File 51\n",
      "File 53   and File 52\n",
      "File 54   and File 53\n",
      "File 55   and File 54\n",
      "File 56   and File 55\n",
      "File 57   and File 56\n",
      "File 58   and File 57\n",
      "File 59   and File 58\n",
      "File 60   and File 59\n",
      "File 61   and File 60\n",
      "File 62   and File 61\n",
      "File 63   and File 62\n",
      "File 64   and File 63\n",
      "File 65   and File 64\n",
      "File 66   and File 65\n",
      "File 67   and File 66\n",
      "File 68   and File 67\n",
      "File 69   and File 68\n",
      "File 70   and File 69\n",
      "File 71   and File 70\n",
      "File 72   and File 71\n",
      "File 73   and File 72\n",
      "File 74   and File 73\n"
     ]
    }
   ],
   "source": [
    "missing_annonces = []\n",
    "for i in range(1, len(annonces_par_fichier)):\n",
    "    print('File', i, '  and File', i-1)\n",
    "    prev_annonce = annonces_par_fichier[i - 1]\n",
    "    curr_annonce = annonces_par_fichier[i]\n",
    "    common_keys = set(prev_annonce.keys()) & set(curr_annonce.keys())\n",
    "    missing_keys = set(prev_annonce.keys()) - set(curr_annonce.keys())\n",
    "\n",
    "    for key in missing_keys:\n",
    "        annonces_par_fichier[i][key] = []\n",
    "\n",
    "    for key in common_keys:\n",
    "        if len(prev_annonce[key]) > len(curr_annonce[key]):\n",
    "            for index in range(len(curr_annonce[key]), len(prev_annonce[key])):\n",
    "                annonce = copy.deepcopy(prev_annonce[key][index])\n",
    "                date_debut = annonce['date_debut']\n",
    "                date_fin = annonce['date_fin']\n",
    "                if(date_debut is not None):\n",
    "                    annonce['date_debut'] = date_debut + timedelta(days=7)\n",
    "                    annonce['date_fin'] = date_fin + timedelta(days=7)\n",
    "                    annonce['row']['date_debut'] = add_days(7, annonce['row']['date_debut'])\n",
    "                    annonce['row']['date_fin'] = add_days(7, annonce['row']['date_fin'])\n",
    "                    missing_annonces.append(annonce)\n",
    "                    annonces_par_fichier[i][key].append(annonce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_missing_annonces(missing_annonces, file_name):\n",
    "    print('Missings ', len(missing_annonces)) \n",
    "    \n",
    "    # Get the headers from the first row, but filter out the 'Unnamed: 12' column\n",
    "    header = [field for field in missing_annonces[0]['row'].keys() if field != 'Unnamed: 12']\n",
    "    print(header)\n",
    "    \n",
    "    # Writing to CSV\n",
    "    with open(file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        for item in missing_annonces:\n",
    "            # Remove the 'Unnamed: 12' column if it exists in the row\n",
    "            row = {key: value for key, value in item['row'].items() if key in header}\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missings  15847892\n",
      "['web-scraper-order', 'date_price', 'date_debut', 'date_fin', 'prix_init', 'prix_actuel', 'typologie', 'n_offre', 'nom', 'localite', 'date_debut-jour', 'Nb semaines']\n"
     ]
    }
   ],
   "source": [
    "write_missing_annonces(missing_annonces,'booking_3j_missings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_missing_annonces(missing_annonces,'maeva_missings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'annonces_par_fichier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Creating hosting DB for test\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, prev_annonce \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mannonces_par_fichier\u001b[49m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m         keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(prev_annonce\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'annonces_par_fichier' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating hosting DB for test\n",
    "for i, prev_annonce in enumerate(annonces_par_fichier):\n",
    "    if i == 0:\n",
    "        keys = set(prev_annonce.keys())\n",
    "        key = next(iter(keys))\n",
    "    print('File:',i)    \n",
    "    for index in range(len(prev_annonce[key])):\n",
    "        annonce = prev_annonce[key][index]\n",
    "        header = [field for field in annonce['row'].keys() if field != 'Unnamed: 12']\n",
    "        \n",
    "        with open(f'test/{key}_{i}.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=header)\n",
    "            if file.tell() == 0:  \n",
    "                writer.writeheader()\n",
    "            row = {k: v for k, v in annonce['row'].items() if k in header}\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
